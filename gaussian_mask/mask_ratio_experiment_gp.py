#!/usr/bin/env python3
"""
ã‚¬ã‚¦ã‚¹éç¨‹ãƒ‡ãƒ¼ã‚¿ç”¨ãƒã‚¹ã‚¯ç‡å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ä½¿ã„æ–¹: python mask_ratio_experiment_gp.py --smoothness 5 --mask_ratios 0.2 0.3 --epochs 500
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split
import time
import os
import argparse
import datetime
import pandas as pd
from typing import List, Tuple, Any, Dict

# å…±é€šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from models import DeepONetDataset, create_deeponet_model, calculate_l2_relative_error, calculate_statistics
from config import DEVICE, MODEL_CONFIG, TRAINING_CONFIG

def get_gp_paths(smoothness):
    """ã‚¬ã‚¦ã‚¹éç¨‹ãƒ‡ãƒ¼ã‚¿ç”¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ã‚’å–å¾—"""
    base_data_dir = f'./data_gp_smooth_{smoothness}'
    result_dir = f'./result_gp_smooth_{smoothness}_mask_experiment'
    os.makedirs(result_dir, exist_ok=True)
    return base_data_dir, result_dir

class GaussianProcessMaskExperiment:
    """ã‚¬ã‚¦ã‚¹éç¨‹ãƒ‡ãƒ¼ã‚¿ç”¨ãƒã‚¹ã‚¯ç‡å®Ÿé¨“ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, smoothness: int):
        self.smoothness = smoothness
        self.base_data_dir, self.result_dir = get_gp_paths(smoothness)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
        branch_file = f"{self.base_data_dir}/deeponet_branch.npy"
        if not os.path.exists(branch_file):
            raise FileNotFoundError(
                f"ã‚¬ã‚¦ã‚¹éç¨‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {branch_file}\n"
                f"å…ˆã«ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:\n"
                f"python gp_data_generator.py --smoothness {smoothness} --samples 500 --nx 100"
            )
        
        # å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ä¸€åº¦ã ã‘èª­ã¿è¾¼ã¿
        self.branch_data = np.load(f"{self.base_data_dir}/deeponet_branch.npy")
        self.trunk_coords = np.load(f"{self.base_data_dir}/deeponet_trunk.npy") 
        self.target_data = np.load(f"{self.base_data_dir}/deeponet_target.npy")
        
        self.num_samples, self.Nx = self.branch_data.shape
        
        # ãƒ†ã‚¹ãƒˆç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å›ºå®šï¼ˆä¸€è²«ã—ãŸè©•ä¾¡ã®ãŸã‚ï¼‰
        sample_indices = np.arange(self.num_samples)
        self.train_sample_idx, self.test_sample_idx = train_test_split(
            sample_indices, test_size=0.2, random_state=42
        )
        
        print(f"ã‚¬ã‚¦ã‚¹éç¨‹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {self.branch_data.shape}")
        print(f"Smoothness Level: {smoothness}")
        print(f"è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«: {len(self.train_sample_idx)}, ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«: {len(self.test_sample_idx)}")
        
        # ãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ€§ã‚’è¡¨ç¤º
        self.analyze_data_characteristics()
    
    def analyze_data_characteristics(self):
        """ãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ€§ã‚’åˆ†æè¡¨ç¤º"""
        # æ»‘ã‚‰ã‹ã•ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        gradients = []
        curvatures = []
        for i in range(min(50, self.num_samples)):
            u = self.branch_data[i]
            grad = np.gradient(u, self.trunk_coords)
            curvature = np.gradient(grad, self.trunk_coords)
            gradients.append(np.std(grad))
            curvatures.append(np.std(curvature))
        
        print(f"ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§åˆ†æ:")
        print(f"  - ãƒ‡ãƒ¼ã‚¿ç¯„å›²: [{np.min(self.branch_data):.3f}, {np.max(self.branch_data):.3f}]")
        print(f"  - å‹¾é…æ¨™æº–åå·®: {np.mean(gradients):.3f}")
        print(f"  - æ›²ç‡æ¨™æº–åå·®: {np.mean(curvatures):.3f}")
        print(f"  - æœ€å¤§éš£æ¥å·®åˆ†: {np.mean([np.max(np.abs(np.diff(self.branch_data[i]))) for i in range(min(50, self.num_samples))]):.3f}")
    
    def create_mask_indices(self, mask_ratio: float) -> Tuple[np.ndarray, int, int]:
        """ãƒã‚¹ã‚¯ç”¨ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ"""
        mask_points = int(self.Nx * mask_ratio)
        unmask_start = mask_points
        unmask_end = self.Nx - mask_points
        eval_indices = np.arange(unmask_start, unmask_end)
        return eval_indices, unmask_start, unmask_end
    
    def prepare_data(self, mask_ratio: float, case: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray, int]:
        """æŒ‡å®šã•ã‚ŒãŸãƒã‚¹ã‚¯ç‡ã¨ã‚±ãƒ¼ã‚¹ã§ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™"""
        eval_indices, unmask_start, unmask_end = self.create_mask_indices(mask_ratio)
        
        # ãƒã‚¹ã‚¯è¨­å®šã®è¡¨ç¤º
        mask_points = int(self.Nx * mask_ratio)
        print(f"ãƒã‚¹ã‚¯è¨­å®š:")
        print(f"  å…¨ä½“ã‚°ãƒªãƒƒãƒ‰ç‚¹æ•°: {self.Nx}")
        print(f"  ãƒã‚¹ã‚¯ç‚¹æ•°ï¼ˆå„ç«¯ï¼‰: {mask_points}")
        print(f"  è©•ä¾¡é ˜åŸŸ: [{unmask_start}, {unmask_end}) ({len(eval_indices)}ç‚¹)")
        
        if case == "case1":
            print(f"=== ã‚±ãƒ¼ã‚¹1: åˆæœŸå€¤å…¨åŸŸ + äºˆæ¸¬ä¸­å¤®é ˜åŸŸ ===")
            # ã‚±ãƒ¼ã‚¹1: åˆæœŸå€¤å…¨åŸŸ, äºˆæ¸¬ä¸­å¤®
            branch_input = []
            trunk_input = []
            target_output = []
            
            for i in range(self.num_samples):
                for j in eval_indices:
                    branch_input.append(self.branch_data[i])  # å…¨åŸŸ
                    trunk_input.append([self.trunk_coords[j]])
                    target_output.append([self.target_data[i, j]])
                    
            branch_dim = self.Nx
            
        elif case == "case2":
            print(f"=== ã‚±ãƒ¼ã‚¹2: åˆæœŸå€¤ä¸­å¤®éƒ¨åˆ† + äºˆæ¸¬ä¸­å¤®é ˜åŸŸ ===")
            # ã‚±ãƒ¼ã‚¹2: åˆæœŸå€¤ãƒã‚¹ã‚¯, äºˆæ¸¬ä¸­å¤®
            branch_data_masked = self.branch_data[:, eval_indices]
            branch_input = []
            trunk_input = []
            target_output = []
            
            for i in range(self.num_samples):
                for j in eval_indices:
                    branch_input.append(branch_data_masked[i])  # ãƒã‚¹ã‚¯å¾Œ
                    trunk_input.append([self.trunk_coords[j]])
                    target_output.append([self.target_data[i, j]])
            
            branch_dim = len(eval_indices)
        
        else:
            raise ValueError("case must be 'case1' or 'case2'")
        
        branch_input = np.array(branch_input, dtype=np.float32)
        trunk_input = np.array(trunk_input, dtype=np.float32)  
        target_output = np.array(target_output, dtype=np.float32)
        
        # ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ã®è¡¨ç¤º
        if case == "case1":
            print(f"ã‚±ãƒ¼ã‚¹1ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶:")
            print(f"  Branch input: {branch_input.shape} (å…¨åŸŸåˆæœŸæ¡ä»¶)")
        else:
            print(f"ã‚±ãƒ¼ã‚¹2ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶:")
            print(f"  Branch input: {branch_input.shape} (ãƒã‚¹ã‚¯åˆæœŸæ¡ä»¶)")
        print(f"  Trunk input: {trunk_input.shape}")
        print(f"  Target output: {target_output.shape}")
        
        return branch_input, trunk_input, target_output, branch_dim
    
    def train_model(self, mask_ratio: float, case: str, num_epochs: int = None) -> Dict[str, Any]:
        """æŒ‡å®šã•ã‚ŒãŸãƒã‚¹ã‚¯ç‡ã¨ã‚±ãƒ¼ã‚¹ã§ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’"""
        
        if num_epochs is None:
            num_epochs = TRAINING_CONFIG['epochs']
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        branch_input, trunk_input, target_output, branch_dim = self.prepare_data(mask_ratio, case)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆï¼ˆå›ºå®šã•ã‚ŒãŸtrain/teståˆ†å‰²ã‚’ä½¿ç”¨ï¼‰
        train_data_indices = []
        test_data_indices = []
        eval_indices, _, _ = self.create_mask_indices(mask_ratio)
        
        for i, sample_idx in enumerate(self.train_sample_idx):
            start_idx = sample_idx * len(eval_indices)
            end_idx = (sample_idx + 1) * len(eval_indices)
            train_data_indices.extend(range(start_idx, end_idx))
        
        for i, sample_idx in enumerate(self.test_sample_idx):
            start_idx = sample_idx * len(eval_indices) 
            end_idx = (sample_idx + 1) * len(eval_indices)
            test_data_indices.extend(range(start_idx, end_idx))
        
        train_dataset = DeepONetDataset(
            branch_input[train_data_indices], 
            trunk_input[train_data_indices], 
            target_output[train_data_indices]
        )
        test_dataset = DeepONetDataset(
            branch_input[test_data_indices],
            trunk_input[test_data_indices], 
            target_output[test_data_indices]
        )
        
        train_loader = DataLoader(train_dataset, batch_size=TRAINING_CONFIG['batch_size'], shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=TRAINING_CONFIG['batch_size'], shuffle=False)
        
        print(f"ãƒ‡ãƒ¼ã‚¿åˆ†å‰²: è¨“ç·´ {len(train_dataset)}, ãƒ†ã‚¹ãƒˆ {len(test_dataset)}")
        print(f"  {case} å­¦ç¿’é–‹å§‹...")
        
        # ãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆå…±é€šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä½¿ç”¨ï¼‰
        model = create_deeponet_model(
            branch_input_dim=branch_dim,
            **MODEL_CONFIG,
            device=DEVICE
        )
        
        # å­¦ç¿’è¨­å®š
        criterion = nn.MSELoss()
        optimizer = optim.Adam(
            model.parameters(), 
            lr=TRAINING_CONFIG['learning_rate'],
            weight_decay=TRAINING_CONFIG['weight_decay']
        )
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.9)
        
        train_losses = []
        test_losses = []
        best_test_loss = float('inf')
        
        start_time = time.time()
        
        for epoch in range(num_epochs):
            # è¨“ç·´
            model.train()
            train_loss = 0.0
            
            for batch_branch, batch_trunk, batch_target in train_loader:
                batch_branch = batch_branch.to(DEVICE)
                batch_trunk = batch_trunk.to(DEVICE)
                batch_target = batch_target.to(DEVICE)
                
                optimizer.zero_grad()
                outputs = model(batch_branch, batch_trunk)
                loss = criterion(outputs, batch_target)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
            
            train_loss /= len(train_loader)
            train_losses.append(train_loss)
            
            # ãƒ†ã‚¹ãƒˆ
            model.eval()
            test_loss = 0.0
            
            with torch.no_grad():
                for batch_branch, batch_trunk, batch_target in test_loader:
                    batch_branch = batch_branch.to(DEVICE)
                    batch_trunk = batch_trunk.to(DEVICE)
                    batch_target = batch_target.to(DEVICE)
                    
                    outputs = model(batch_branch, batch_trunk)
                    loss = criterion(outputs, batch_target)
                    test_loss += loss.item()
            
            test_loss /= len(test_loader)
            test_losses.append(test_loss)
            
            scheduler.step()
            
            if test_loss < best_test_loss:
                best_test_loss = test_loss
                model_path = f'{self.result_dir}/best_model_{case}_mask{int(mask_ratio*100)}.pth'
                torch.save(model.state_dict(), model_path)
            
            if (epoch + 1) % 100 == 0 or epoch == 0:
                elapsed = time.time() - start_time
                print(f'    Epoch [{epoch+1:4d}/{num_epochs}] | '
                      f'Train: {train_loss:.6f} | Test: {test_loss:.6f} | Time: {elapsed:.1f}s')
        
        print(f"  {case} å­¦ç¿’å®Œäº†! æœ€è‰¯ãƒ†ã‚¹ãƒˆæå¤±: {best_test_loss:.6f}")
        
        return {
            'model': model,
            'train_losses': train_losses,
            'test_losses': test_losses, 
            'best_test_loss': best_test_loss,
            'branch_dim': branch_dim,
            'eval_indices': eval_indices
        }
    
    def evaluate_model(self, mask_ratio: float, case: str, model_info: Dict[str, Any]) -> np.ndarray:
        """ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã¦ã‚¨ãƒ©ãƒ¼ã‚’è¨ˆç®—ï¼ˆå…±é€šé–¢æ•°ä½¿ç”¨ï¼‰"""
        eval_indices = model_info['eval_indices']
        model = model_info['model']
        model.eval()
        
        errors = []
        
        if case == "case1":
            branch_data_input = self.branch_data
        else:  # case2
            branch_data_input = self.branch_data[:, eval_indices]
        
        with torch.no_grad():
            for sample_idx in self.test_sample_idx:
                # çœŸã®å€¤ï¼ˆè©•ä¾¡é ˜åŸŸï¼‰
                true_values = self.target_data[sample_idx, eval_indices]
                
                # äºˆæ¸¬
                branch_input = torch.FloatTensor(branch_data_input[sample_idx]).unsqueeze(0).repeat(len(eval_indices), 1).to(DEVICE)
                trunk_input = torch.FloatTensor(self.trunk_coords[eval_indices]).unsqueeze(1).to(DEVICE)
                pred_values = model(branch_input, trunk_input).cpu().numpy().flatten()
                
                # ç›¸å¯¾L2èª¤å·®ï¼ˆå…±é€šé–¢æ•°ä½¿ç”¨ï¼‰
                l2_error = calculate_l2_relative_error(true_values, pred_values)
                errors.append(l2_error)
        
        return np.array(errors)
    
    def run_experiment(self, mask_ratios: List[float], num_epochs: int = None) -> pd.DataFrame:
        """è¤‡æ•°ã®ãƒã‚¹ã‚¯ç‡ã§å®Ÿé¨“ã‚’å®Ÿè¡Œ"""
        if num_epochs is None:
            num_epochs = TRAINING_CONFIG['epochs']
            
        print(f"=== ã‚¬ã‚¦ã‚¹éç¨‹ Smoothness {self.smoothness} ãƒã‚¹ã‚¯ç‡å®Ÿé¨“é–‹å§‹ ===")
        print(f"ãƒã‚¹ã‚¯ç‡: {[r*100 for r in mask_ratios]}%")
        print(f"ã‚¨ãƒãƒƒã‚¯æ•°: {num_epochs}")
        print(f"çµæœä¿å­˜å…ˆ: {self.result_dir}")
        
        results = []
        
        for mask_ratio in mask_ratios:
            print(f"\n{'='*50}")
            print(f"ãƒã‚¹ã‚¯ç‡ {mask_ratio*100}% å®Ÿé¨“")
            print(f"{'='*50}")
            
            # Case1ã¨Case2ã‚’å­¦ç¿’
            case1_info = self.train_model(mask_ratio, "case1", num_epochs)
            case2_info = self.train_model(mask_ratio, "case2", num_epochs)
            
            # è©•ä¾¡
            case1_errors = self.evaluate_model(mask_ratio, "case1", case1_info)
            case2_errors = self.evaluate_model(mask_ratio, "case2", case2_info)
            
            # çµ±è¨ˆè¨ˆç®—ï¼ˆå…±é€šé–¢æ•°ä½¿ç”¨ï¼‰
            case1_stats = calculate_statistics(case1_errors)
            case2_stats = calculate_statistics(case2_errors)
            
            # æ”¹å–„ç‡è¨ˆç®—
            improvement = (case1_stats['mean'] - case2_stats['mean']) / case1_stats['mean'] * 100
            
            result = {
                'Smoothness': self.smoothness,
                'Mask_Ratio': mask_ratio,
                'Eval_Region_Pct': (1 - 2*mask_ratio) * 100,
                'Eval_Points': len(case1_info['eval_indices']),
                'Case1_Loss': case1_info['best_test_loss'],
                'Case2_Loss': case2_info['best_test_loss'],
                'Loss_Ratio': case2_info['best_test_loss'] / case1_info['best_test_loss'],
                'Case1_Mean_Error': case1_stats['mean'],
                'Case1_Std_Error': case1_stats['std'],
                'Case2_Mean_Error': case2_stats['mean'],
                'Case2_Std_Error': case2_stats['std'],
                'Improvement_Rate': improvement,
                'Case1_Input_Dim': case1_info['branch_dim'],
                'Case2_Input_Dim': case2_info['branch_dim']
            }
            
            results.append(result)
            
            print(f"\nãƒã‚¹ã‚¯ç‡ {mask_ratio*100}% çµæœ:")
            print(f"  Case1 èª¤å·®: {case1_stats['mean']:.6f} Â± {case1_stats['stderr']:.6f} (SE)")
            print(f"  Case2 èª¤å·®: {case2_stats['mean']:.6f} Â± {case2_stats['stderr']:.6f} (SE)")
            print(f"  ç›¸å¯¾å¤‰åŒ–ç‡: {improvement:.2f}%")
            
            # å­¦ç¿’æˆåŠŸ/å¤±æ•—ã®åˆ¤å®š
            if case2_stats['mean'] > 1.0:  # ç›¸å¯¾èª¤å·®100%ä»¥ä¸Š
                print(f"  âš ï¸  Case2å­¦ç¿’å›°é›£: ç›¸å¯¾èª¤å·® {case2_stats['mean']:.3f}")
            elif case2_stats['mean'] > 0.5:  # ç›¸å¯¾èª¤å·®50%ä»¥ä¸Š
                print(f"  ğŸŸ¡ Case2å­¦ç¿’éƒ¨åˆ†çš„: ç›¸å¯¾èª¤å·® {case2_stats['mean']:.3f}")
            else:
                print(f"  âœ… Case2å­¦ç¿’æˆåŠŸ: ç›¸å¯¾èª¤å·® {case2_stats['mean']:.3f}")
        
        return pd.DataFrame(results)

def main():
    parser = argparse.ArgumentParser(description='Gaussian Process Mask Ratio Experiment')
    parser.add_argument('--smoothness', type=int, default=5, help='Smoothness level (0:smooth â†’ 10:sharp)')
    parser.add_argument('--epochs', type=int, default=None, help=f'Number of epochs (default: {TRAINING_CONFIG["epochs"]})')
    parser.add_argument('--mask_ratios', nargs='+', type=float, 
                       default=[0.2, 0.3], 
                       help='List of mask ratios to test')
    args = parser.parse_args()
    
    if not (0 <= args.smoothness <= 10):
        print("Warning: smoothness level should be between 0-10")
        print("  0-3: RBF kernel (infinitely smooth)")
        print("  4-6: MatÃ©rn 5/2 kernel (moderately smooth)")  
        print("  7-8: MatÃ©rn 3/2 kernel (less smooth)")
        print("  9-10: MatÃ©rn 1/2 kernel (sharp)")
    
    print("="*60)
    print(f"ã‚¬ã‚¦ã‚¹éç¨‹ãƒã‚¹ã‚¯ç‡å®Ÿé¨“")
    print("="*60)
    print(f"Smoothness Level: {args.smoothness}")
    print(f"ãƒã‚¹ã‚¯ç‡: {[r*100 for r in args.mask_ratios]}%")
    print(f"ã‚¨ãƒãƒƒã‚¯æ•°: {args.epochs or TRAINING_CONFIG['epochs']}")
    print(f"ãƒ‡ãƒã‚¤ã‚¹: {DEVICE}")
    
    try:
        # å®Ÿé¨“å®Ÿè¡Œ
        experiment = GaussianProcessMaskExperiment(args.smoothness)
        results_df = experiment.run_experiment(args.mask_ratios, args.epochs)
        
        # çµæœä¿å­˜
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        csv_path = f'{experiment.result_dir}/gp_mask_results_{timestamp}.csv'
        results_df.to_csv(csv_path, index=False, encoding='utf-8')
        
        # ç°¡æ½”ãªã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print(f"\n{'='*60}")
        print("å®Ÿé¨“å®Œäº†ã‚µãƒãƒªãƒ¼")
        print(f"{'='*60}")
        print(results_df.to_string(index=False, float_format='%.4f'))
        print(f"\nçµæœãƒ•ã‚¡ã‚¤ãƒ«: {csv_path}")
        print("="*60)
        
    except FileNotFoundError as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"\nè§£æ±ºæ–¹æ³•:")
        print(f"python gp_data_generator.py --smoothness {args.smoothness} --samples 500 --nx 100")

if __name__ == "__main__":
    main()