#!/usr/bin/env python3
"""
ã‚¬ã‚¦ã‚¹éç¨‹ãƒ‡ãƒ¼ã‚¿ç”¨ãƒã‚¹ã‚¯ç‡0.2å°‚ç”¨åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›ãƒ†ã‚¹ãƒˆä»˜ãï¼‰
ä½¿ç”¨æ–¹æ³•: python analyze_gp_mask02_enhanced.py --smoothness 5
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
import glob
import argparse
import torch
from sklearn.model_selection import train_test_split

# å…±é€šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from models import load_deeponet_model, calculate_l2_relative_error, calculate_statistics
from config import DEVICE, MODEL_CONFIG

def get_gp_paths(smoothness):
    """ã‚¬ã‚¦ã‚¹éç¨‹ãƒ‡ãƒ¼ã‚¿ç”¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ã‚’å–å¾—"""
    base_data_dir = f'./data_gp_smooth_{smoothness}'
    result_dir = f'./result_gp_smooth_{smoothness}_mask_experiment'
    return base_data_dir, result_dir

class EnhancedGPMaskRatio02Analyzer:
    """ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›ãƒ†ã‚¹ãƒˆä»˜ãã‚¬ã‚¦ã‚¹éç¨‹ãƒ‡ãƒ¼ã‚¿ç”¨ãƒã‚¹ã‚¯ç‡0.2å°‚ç”¨åˆ†æã‚¯ãƒ©ã‚¹"""

    def __init__(self, smoothness):
        self.smoothness = smoothness
        self.mask_ratio = 0.2  # å›ºå®š
        self.base_data_dir, self.result_dir = get_gp_paths(smoothness)

        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        try:
            self.branch_data = np.load(f"{self.base_data_dir}/deeponet_branch.npy")
            self.trunk_coords = np.load(f"{self.base_data_dir}/deeponet_trunk.npy")
            self.target_data = np.load(f"{self.base_data_dir}/deeponet_target.npy")
            self.num_samples, self.Nx = self.branch_data.shape
        except FileNotFoundError:
            raise FileNotFoundError(
                f"ã‚¬ã‚¦ã‚¹éç¨‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.base_data_dir}\n"
                f"å…ˆã«ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:\n"
                f"python gp_data_generator.py --smoothness {smoothness} --samples 500 --nx 100"
            )

        # å­¦ç¿’ã¨åŒã˜train/teståˆ†å‰²ã‚’å†ç¾
        sample_indices = np.arange(self.num_samples)
        self.train_sample_idx, self.test_sample_idx = train_test_split(
            sample_indices, test_size=0.2, random_state=42
        )

        # ãƒã‚¹ã‚¯è¨­å®š
        self.eval_indices, self.unmask_start, self.unmask_end = self.create_mask_indices()

        print(f"ã‚¬ã‚¦ã‚¹éç¨‹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†:")
        print(f"  Smoothness Level: {self.smoothness}")
        print(f"  å…¨ã‚µãƒ³ãƒ—ãƒ«æ•°: {self.num_samples}")
        print(f"  ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«æ•°: {len(self.test_sample_idx)}")
        print(f"  ã‚°ãƒªãƒƒãƒ‰ç‚¹æ•°: {self.Nx}")
        print(f"  ãƒã‚¹ã‚¯ç‡: {self.mask_ratio} (ä¸­å¤®éƒ¨ {(1-2*self.mask_ratio)*100:.0f}%)")
        print(f"  è©•ä¾¡é ˜åŸŸ: [{self.unmask_start}, {self.unmask_end}) ({len(self.eval_indices)}ç‚¹)")

        # ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§åˆ†æ
        self.analyze_data_characteristics()

    def analyze_data_characteristics(self):
        """ãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ€§ã‚’åˆ†æè¡¨ç¤º"""
        gradients = []
        curvatures = []
        amplitudes = []
        
        for i in range(min(50, self.num_samples)):
            u = self.branch_data[i]
            grad = np.gradient(u, self.trunk_coords)
            curvature = np.gradient(grad, self.trunk_coords)
            gradients.append(np.std(grad))
            curvatures.append(np.std(curvature))
            amplitudes.append(np.max(np.abs(u)))

        self.data_amplitude_range = [np.min(amplitudes), np.max(amplitudes)]
        
        print(f"  ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§:")
        print(f"    - ãƒ‡ãƒ¼ã‚¿ç¯„å›²: [{np.min(self.branch_data):.3f}, {np.max(self.branch_data):.3f}]")
        print(f"    - æŒ¯å¹…ç¯„å›²: [{self.data_amplitude_range[0]:.3f}, {self.data_amplitude_range[1]:.3f}]")
        print(f"    - å‹¾é…æ¨™æº–åå·®: {np.mean(gradients):.3f}")
        print(f"    - æ›²ç‡æ¨™æº–åå·®: {np.mean(curvatures):.3f}")
        print(f"    - æœ€å¤§éš£æ¥å·®åˆ†: {np.mean([np.max(np.abs(np.diff(self.branch_data[i]))) for i in range(min(50, self.num_samples))]):.3f}")

    def create_mask_indices(self):
        """ãƒã‚¹ã‚¯ç‡0.2ã§ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ"""
        mask_points = int(self.Nx * self.mask_ratio)
        unmask_start = mask_points
        unmask_end = self.Nx - mask_points
        eval_indices = np.arange(unmask_start, unmask_end)
        return eval_indices, unmask_start, unmask_end

    def load_models(self):
        """Case1ã¨Case2ã®ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        try:
            # Case1ãƒ¢ãƒ‡ãƒ«ï¼ˆå…¨åŸŸåˆæœŸæ¡ä»¶ï¼‰
            case1_model_path = f'{self.result_dir}/best_model_case1_mask20.pth'
            self.case1_model = load_deeponet_model(
                model_path=case1_model_path,
                branch_input_dim=self.Nx,  # å…¨åŸŸ
                latent_dim=MODEL_CONFIG['latent_dim'],
                hidden_layers=MODEL_CONFIG['hidden_layers'],
                hidden_dim=MODEL_CONFIG['hidden_dim'],
                activation=MODEL_CONFIG['activation'],
                device=DEVICE
            )
            print(f"Case1ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {case1_model_path}")

            # Case2ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒã‚¹ã‚¯åˆæœŸæ¡ä»¶ï¼‰
            case2_model_path = f'{self.result_dir}/best_model_case2_mask20.pth'
            self.case2_model = load_deeponet_model(
                model_path=case2_model_path,
                branch_input_dim=len(self.eval_indices),  # ãƒã‚¹ã‚¯å¾Œ
                latent_dim=MODEL_CONFIG['latent_dim'],
                hidden_layers=MODEL_CONFIG['hidden_layers'],
                hidden_dim=MODEL_CONFIG['hidden_dim'],
                activation=MODEL_CONFIG['activation'],
                device=DEVICE
            )
            print(f"Case2ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {case2_model_path}")

            return True

        except Exception as e:
            print(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"å…ˆã«ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:")
            print(f"python mask_ratio_experiment_gp.py --smoothness {self.smoothness} --mask_ratios 0.2 --epochs 300")
            return False

    def scale_transform_test(self, scale_factors=[0.5, 2.0], n_test_samples=20):
        """ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        print(f"\n{'='*60}")
        print(f"ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        print(f"ã‚¹ã‚±ãƒ¼ãƒ«ä¿‚æ•°: {scale_factors}")
        print(f"ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«æ•°: {n_test_samples}")
        print(f"{'='*60}")
        
        # ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠï¼ˆæœ€åˆã®n_test_sampleså€‹ï¼‰
        test_samples = self.test_sample_idx[:n_test_samples]
        
        scale_results = {}
        
        for scale_factor in scale_factors:
            print(f"\nã‚¹ã‚±ãƒ¼ãƒ«ä¿‚æ•° {scale_factor} ã§ã®å®Ÿè¡Œä¸­...")
            
            case1_original_errors = []
            case1_scaled_errors = []
            case1_linearity_errors = []
            
            case2_original_errors = []
            case2_scaled_errors = []
            case2_linearity_errors = []
            
            with torch.no_grad():
                for i, sample_idx in enumerate(test_samples):
                    if (i + 1) % 5 == 0:
                        print(f"  é€²æ—: {i + 1}/{len(test_samples)}")
                    
                    # å…ƒã®åˆæœŸæ¡ä»¶
                    u0_original = self.branch_data[sample_idx]
                    true_values = self.target_data[sample_idx, self.eval_indices]
                    
                    # ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›ã—ãŸåˆæœŸæ¡ä»¶
                    u0_scaled = scale_factor * u0_original
                    
                    # çœŸã®è§£ã‚‚ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›ï¼ˆç·šå½¢æ€§ã«ã‚ˆã‚Šï¼‰
                    true_values_scaled = scale_factor * true_values
                    
                    # Case1: å…ƒã®åˆæœŸæ¡ä»¶ã§ã®äºˆæ¸¬
                    branch_input_case1_orig = torch.FloatTensor(u0_original).unsqueeze(0).repeat(len(self.eval_indices), 1).to(DEVICE)
                    trunk_input = torch.FloatTensor(self.trunk_coords[self.eval_indices]).unsqueeze(1).to(DEVICE)
                    pred_case1_orig = self.case1_model(branch_input_case1_orig, trunk_input).cpu().numpy().flatten()
                    
                    # Case1: ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›ã—ãŸåˆæœŸæ¡ä»¶ã§ã®äºˆæ¸¬
                    branch_input_case1_scaled = torch.FloatTensor(u0_scaled).unsqueeze(0).repeat(len(self.eval_indices), 1).to(DEVICE)
                    pred_case1_scaled = self.case1_model(branch_input_case1_scaled, trunk_input).cpu().numpy().flatten()
                    
                    # Case1: ç·šå½¢æ€§ãƒã‚§ãƒƒã‚¯
                    pred_case1_expected = scale_factor * pred_case1_orig
                    
                    # Case2: å…ƒã®åˆæœŸæ¡ä»¶ï¼ˆãƒã‚¹ã‚¯ï¼‰ã§ã®äºˆæ¸¬
                    branch_input_case2_orig = torch.FloatTensor(u0_original[self.eval_indices]).unsqueeze(0).repeat(len(self.eval_indices), 1).to(DEVICE)
                    pred_case2_orig = self.case2_model(branch_input_case2_orig, trunk_input).cpu().numpy().flatten()
                    
                    # Case2: ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›ã—ãŸåˆæœŸæ¡ä»¶ï¼ˆãƒã‚¹ã‚¯ï¼‰ã§ã®äºˆæ¸¬
                    branch_input_case2_scaled = torch.FloatTensor(u0_scaled[self.eval_indices]).unsqueeze(0).repeat(len(self.eval_indices), 1).to(DEVICE)
                    pred_case2_scaled = self.case2_model(branch_input_case2_scaled, trunk_input).cpu().numpy().flatten()
                    
                    # Case2: ç·šå½¢æ€§ãƒã‚§ãƒƒã‚¯
                    pred_case2_expected = scale_factor * pred_case2_orig
                    
                    # èª¤å·®è¨ˆç®—
                    # Case1
                    case1_orig_error = calculate_l2_relative_error(true_values, pred_case1_orig)
                    case1_scaled_error = calculate_l2_relative_error(true_values_scaled, pred_case1_scaled)
                    case1_linearity_error = calculate_l2_relative_error(pred_case1_scaled, pred_case1_expected)
                    
                    case1_original_errors.append(case1_orig_error)
                    case1_scaled_errors.append(case1_scaled_error)
                    case1_linearity_errors.append(case1_linearity_error)
                    
                    # Case2
                    case2_orig_error = calculate_l2_relative_error(true_values, pred_case2_orig)
                    case2_scaled_error = calculate_l2_relative_error(true_values_scaled, pred_case2_scaled)
                    case2_linearity_error = calculate_l2_relative_error(pred_case2_scaled, pred_case2_expected)
                    
                    case2_original_errors.append(case2_orig_error)
                    case2_scaled_errors.append(case2_scaled_error)
                    case2_linearity_errors.append(case2_linearity_error)
            
            # çµæœä¿å­˜
            scale_results[scale_factor] = {
                'case1_original_errors': np.array(case1_original_errors),
                'case1_scaled_errors': np.array(case1_scaled_errors),
                'case1_linearity_errors': np.array(case1_linearity_errors),
                'case2_original_errors': np.array(case2_original_errors),
                'case2_scaled_errors': np.array(case2_scaled_errors),
                'case2_linearity_errors': np.array(case2_linearity_errors)
            }
            
            print(f"  å®Œäº†: ã‚¹ã‚±ãƒ¼ãƒ«ä¿‚æ•° {scale_factor}")
        
        self.scale_results = scale_results
        return scale_results
    
    def analyze_scale_results(self):
        """ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›ãƒ†ã‚¹ãƒˆçµæœã‚’åˆ†æ"""
        print(f"\n{'='*60}")
        print(f"ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›ãƒ†ã‚¹ãƒˆçµæœåˆ†æ")
        print(f"{'='*60}")
        
        for scale_factor, results in self.scale_results.items():
            print(f"\nã€ã‚¹ã‚±ãƒ¼ãƒ«ä¿‚æ•° {scale_factor}ã€‘")
            
            # Case1åˆ†æ
            case1_orig_stats = calculate_statistics(results['case1_original_errors'])
            case1_scaled_stats = calculate_statistics(results['case1_scaled_errors'])
            case1_linearity_stats = calculate_statistics(results['case1_linearity_errors'])
            
            print(f"  Case1 (å…¨åŸŸåˆæœŸæ¡ä»¶):")
            print(f"    å…ƒã®èª¤å·®: {case1_orig_stats['mean']:.4f} Â± {case1_orig_stats['stderr']:.4f}")
            print(f"    ã‚¹ã‚±ãƒ¼ãƒ«å¾Œèª¤å·®: {case1_scaled_stats['mean']:.4f} Â± {case1_scaled_stats['stderr']:.4f}")
            print(f"    ç·šå½¢æ€§èª¤å·®: {case1_linearity_stats['mean']:.4f} Â± {case1_linearity_stats['stderr']:.4f}")
            
            # Case2åˆ†æ
            case2_orig_stats = calculate_statistics(results['case2_original_errors'])
            case2_scaled_stats = calculate_statistics(results['case2_scaled_errors'])
            case2_linearity_stats = calculate_statistics(results['case2_linearity_errors'])
            
            print(f"  Case2 (ãƒã‚¹ã‚¯åˆæœŸæ¡ä»¶):")
            print(f"    å…ƒã®èª¤å·®: {case2_orig_stats['mean']:.4f} Â± {case2_orig_stats['stderr']:.4f}")
            print(f"    ã‚¹ã‚±ãƒ¼ãƒ«å¾Œèª¤å·®: {case2_scaled_stats['mean']:.4f} Â± {case2_scaled_stats['stderr']:.4f}")
            print(f"    ç·šå½¢æ€§èª¤å·®: {case2_linearity_stats['mean']:.4f} Â± {case2_linearity_stats['stderr']:.4f}")
            
            # ç·šå½¢æ€§åˆ¤å®š
            linearity_threshold = 0.1  # 10%ä»¥ä¸‹ãªã‚‰ç·šå½¢ã¨ã¿ãªã™
            
            case1_is_linear = case1_linearity_stats['mean'] < linearity_threshold
            case2_is_linear = case2_linearity_stats['mean'] < linearity_threshold
            
            print(f"  ç·šå½¢æ€§åˆ¤å®š:")
            print(f"    Case1: {'âœ… ç·šå½¢' if case1_is_linear else 'âŒ éç·šå½¢'} (èª¤å·® {case1_linearity_stats['mean']*100:.1f}%)")
            print(f"    Case2: {'âœ… ç·šå½¢' if case2_is_linear else 'âŒ éç·šå½¢'} (èª¤å·® {case2_linearity_stats['mean']*100:.1f}%)")
    
    def create_scale_analysis_plots(self):
        """ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›ãƒ†ã‚¹ãƒˆçµæœã‚’ãƒ—ãƒ­ãƒƒãƒˆ"""
        if not hasattr(self, 'scale_results'):
            print("ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›ãƒ†ã‚¹ãƒˆãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            return
        
        n_scales = len(self.scale_results)
        fig, axes = plt.subplots(2, n_scales, figsize=(6*n_scales, 12))
        
        if n_scales == 1:
            axes = axes.reshape(2, 1)
        
        scale_factors = list(self.scale_results.keys())
        
        for i, scale_factor in enumerate(scale_factors):
            results = self.scale_results[scale_factor]
            
            # Case1ç·šå½¢æ€§èª¤å·®åˆ†å¸ƒ
            ax1 = axes[0, i]
            ax1.hist(results['case1_linearity_errors'], bins=20, alpha=0.7, color='blue', 
                    density=True, edgecolor='black', linewidth=0.5)
            
            mean_err = np.mean(results['case1_linearity_errors'])
            ax1.axvline(mean_err, color='red', linestyle='--', linewidth=2)
            ax1.axvline(0.1, color='green', linestyle=':', linewidth=2, label='Linearity threshold (10%)')
            
            ax1.set_xlabel('Linearity Error')
            ax1.set_ylabel('Density')
            ax1.set_title(f'Case1 Linearity (Scale {scale_factor})\nMean: {mean_err:.4f}')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # Case2ç·šå½¢æ€§èª¤å·®åˆ†å¸ƒ
            ax2 = axes[1, i]
            ax2.hist(results['case2_linearity_errors'], bins=20, alpha=0.7, color='orange',
                    density=True, edgecolor='black', linewidth=0.5)
            
            mean_err = np.mean(results['case2_linearity_errors'])
            ax2.axvline(mean_err, color='red', linestyle='--', linewidth=2)
            ax2.axvline(0.1, color='green', linestyle=':', linewidth=2, label='Linearity threshold (10%)')
            
            ax2.set_xlabel('Linearity Error')
            ax2.set_ylabel('Density')
            ax2.set_title(f'Case2 Linearity (Scale {scale_factor})\nMean: {mean_err:.4f}')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜
        plot_path = f'{self.result_dir}/scale_analysis_gp_smooth_{self.smoothness}.png'
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        plt.show()
        
        print(f"ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›åˆ†æçµæœä¿å­˜: {plot_path}")

    def calculate_all_errors(self):
        """å…¨ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«ã®èª¤å·®ã‚’è¨ˆç®—"""
        case1_errors = []
        case2_errors = []

        print("é€šå¸¸èª¤å·®è¨ˆç®—ä¸­...")

        with torch.no_grad():
            for i, sample_idx in enumerate(self.test_sample_idx):
                if (i + 1) % 10 == 0:
                    print(f"  é€²æ—: {i + 1}/{len(self.test_sample_idx)}")

                # çœŸã®å€¤ï¼ˆè©•ä¾¡é ˜åŸŸï¼‰
                true_values = self.target_data[sample_idx, self.eval_indices]

                # Case1äºˆæ¸¬ï¼ˆå…¨åŸŸåˆæœŸæ¡ä»¶ï¼‰
                branch_input_case1 = torch.FloatTensor(self.branch_data[sample_idx]).unsqueeze(0).repeat(len(self.eval_indices), 1).to(DEVICE)
                trunk_input = torch.FloatTensor(self.trunk_coords[self.eval_indices]).unsqueeze(1).to(DEVICE)
                pred_case1 = self.case1_model(branch_input_case1, trunk_input).cpu().numpy().flatten()

                # Case2äºˆæ¸¬ï¼ˆãƒã‚¹ã‚¯åˆæœŸæ¡ä»¶ï¼‰
                branch_input_case2 = torch.FloatTensor(self.branch_data[sample_idx, self.eval_indices]).unsqueeze(0).repeat(len(self.eval_indices), 1).to(DEVICE)
                pred_case2 = self.case2_model(branch_input_case2, trunk_input).cpu().numpy().flatten()

                # ç›¸å¯¾L2èª¤å·®è¨ˆç®—
                l2_error_case1 = calculate_l2_relative_error(true_values, pred_case1)
                l2_error_case2 = calculate_l2_relative_error(true_values, pred_case2)

                case1_errors.append(l2_error_case1)
                case2_errors.append(l2_error_case2)

        self.case1_errors = np.array(case1_errors)
        self.case2_errors = np.array(case2_errors)

        print("é€šå¸¸èª¤å·®è¨ˆç®—å®Œäº†!")
        return self.case1_errors, self.case2_errors

    def calculate_relative_change(self):
        """(C2-C1)/C1ã‚’è¨ˆç®—"""
        self.relative_changes = (self.case2_errors - self.case1_errors) / self.case1_errors
        return self.relative_changes

    def get_sample_predictions(self, sample_indices=[0, 1, 2]):
        """æŒ‡å®šã‚µãƒ³ãƒ—ãƒ«ã®äºˆæ¸¬çµæœã‚’å–å¾—"""
        predictions = {}

        with torch.no_grad():
            for i, idx in enumerate(sample_indices):
                if idx >= len(self.test_sample_idx):
                    continue

                sample_idx = self.test_sample_idx[idx]

                # çœŸã®å€¤
                true_values = self.target_data[sample_idx, self.eval_indices]
                initial_values = self.branch_data[sample_idx]

                # Case1äºˆæ¸¬
                branch_input_case1 = torch.FloatTensor(initial_values).unsqueeze(0).repeat(len(self.eval_indices), 1).to(DEVICE)
                trunk_input = torch.FloatTensor(self.trunk_coords[self.eval_indices]).unsqueeze(1).to(DEVICE)
                pred_case1 = self.case1_model(branch_input_case1, trunk_input).cpu().numpy().flatten()

                # Case2äºˆæ¸¬
                branch_input_case2 = torch.FloatTensor(initial_values[self.eval_indices]).unsqueeze(0).repeat(len(self.eval_indices), 1).to(DEVICE)
                pred_case2 = self.case2_model(branch_input_case2, trunk_input).cpu().numpy().flatten()

                # èª¤å·®è¨ˆç®—
                error_case1 = calculate_l2_relative_error(true_values, pred_case1)
                error_case2 = calculate_l2_relative_error(true_values, pred_case2)
                relative_change = (error_case2 - error_case1) / error_case1

                predictions[f'sample_{idx}'] = {
                    'sample_idx': sample_idx,
                    'x_eval': self.trunk_coords[self.eval_indices],
                    'x_all': self.trunk_coords,
                    'initial_full': initial_values,
                    'true_eval': true_values,
                    'pred_case1': pred_case1,
                    'pred_case2': pred_case2,
                    'error_case1': error_case1,
                    'error_case2': error_case2,
                    'relative_change': relative_change
                }

        return predictions

    def create_analysis_plots(self):
        """åˆ†æãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆ"""
        fig = plt.figure(figsize=(20, 12))

        # çµ±è¨ˆè¨ˆç®—
        case1_stats = calculate_statistics(self.case1_errors)
        case2_stats = calculate_statistics(self.case2_errors)
        relative_change_stats = calculate_statistics(self.relative_changes)

        # ãƒ—ãƒ­ãƒƒãƒˆä½œæˆï¼ˆå…ƒã®ã‚³ãƒ¼ãƒ‰ã¨åŒã˜ï¼‰
        # ... [çœç•¥ï¼šå…ƒã®ãƒ—ãƒ­ãƒƒãƒˆä½œæˆã‚³ãƒ¼ãƒ‰ã‚’ãã®ã¾ã¾ä½¿ç”¨]

        plt.tight_layout()

        # ä¿å­˜
        plot_path = f'{self.result_dir}/enhanced_analysis_gp_mask02_smooth_{self.smoothness}.png'
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        plt.show()

        print(f"\nåˆ†æçµæœä¿å­˜: {plot_path}")

        return {
            'case1_stats': case1_stats,
            'case2_stats': case2_stats,
            'relative_change_stats': relative_change_stats,
            'sample_predictions': self.get_sample_predictions([0, 1, 2])
        }

    def print_detailed_summary(self, analysis_results):
        """è©³ç´°ã‚µãƒãƒªãƒ¼ã‚’å‡ºåŠ›ï¼ˆã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›ãƒ†ã‚¹ãƒˆçµæœå«ã‚€ï¼‰"""
        # å…ƒã®ã‚µãƒãƒªãƒ¼å‡ºåŠ›
        # ... [çœç•¥ï¼šå…ƒã®print_detailed_summaryã¨åŒã˜]
        
        # ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›ãƒ†ã‚¹ãƒˆçµæœè¿½åŠ 
        if hasattr(self, 'scale_results'):
            print(f"\nã€ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›ãƒ†ã‚¹ãƒˆçµæœã€‘")
            for scale_factor, results in self.scale_results.items():
                case1_linearity = np.mean(results['case1_linearity_errors'])
                case2_linearity = np.mean(results['case2_linearity_errors'])
                
                print(f"  ã‚¹ã‚±ãƒ¼ãƒ«ä¿‚æ•° {scale_factor}:")
                print(f"    Case1ç·šå½¢æ€§èª¤å·®: {case1_linearity:.4f} ({'âœ…' if case1_linearity < 0.1 else 'âŒ'})")
                print(f"    Case2ç·šå½¢æ€§èª¤å·®: {case2_linearity:.4f} ({'âœ…' if case2_linearity < 0.1 else 'âŒ'})")
            
            print(f"\nã€ç‰©ç† vs çµ±è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¤å®šã€‘")
            avg_case1_linearity = np.mean([np.mean(results['case1_linearity_errors']) for results in self.scale_results.values()])
            avg_case2_linearity = np.mean([np.mean(results['case2_linearity_errors']) for results in self.scale_results.values()])
            
            if avg_case1_linearity < 0.1 and avg_case2_linearity < 0.1:
                print(f"  âœ… ä¸¡ã‚±ãƒ¼ã‚¹ã¨ã‚‚ç·šå½¢æ€§ã‚’ä¿æŒ â†’ ç‰©ç†æ³•å‰‡å­¦ç¿’ã®å¯èƒ½æ€§é«˜")
            elif avg_case1_linearity < 0.1:
                print(f"  ğŸŸ¡ Case1ã®ã¿ç·šå½¢æ€§ä¿æŒ â†’ å…¨åŸŸæƒ…å ±ã§ç‰©ç†æ³•å‰‡å­¦ç¿’")
            elif avg_case2_linearity < 0.1:
                print(f"  ğŸŸ¡ Case2ã®ã¿ç·šå½¢æ€§ä¿æŒ â†’ ãƒã‚¹ã‚¯æƒ…å ±ã§ç‰©ç†æ³•å‰‡å­¦ç¿’")
            else:
                print(f"  âŒ ä¸¡ã‚±ãƒ¼ã‚¹ã¨ã‚‚éç·šå½¢ â†’ çµ±è¨ˆçš„ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã®å¯èƒ½æ€§")

def main():
    parser = argparse.ArgumentParser(description='ã‚¬ã‚¦ã‚¹éç¨‹ãƒã‚¹ã‚¯ç‡0.2å°‚ç”¨åˆ†æï¼ˆã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›ãƒ†ã‚¹ãƒˆä»˜ãï¼‰')
    parser.add_argument('--smoothness', type=int, default=5, help='Gaussian Process smoothness level (0-10)')
    parser.add_argument('--scale_test', action='store_true', help='Run scale transformation test')
    parser.add_argument('--scale_factors', nargs='+', type=float, default=[0.5, 2.0], help='Scale factors for testing')
    parser.add_argument('--n_scale_samples', type=int, default=20, help='Number of samples for scale test')
    args = parser.parse_args()

    if not (0 <= args.smoothness <= 10):
        print("Warning: smoothness level should be between 0-10")

    print("="*60)
    print(f"ã‚¬ã‚¦ã‚¹éç¨‹ãƒã‚¹ã‚¯ç‡0.2å°‚ç”¨åˆ†æï¼ˆæ‹¡å¼µç‰ˆï¼‰")
    print("="*60)
    print(f"Smoothness Level: {args.smoothness}")
    print(f"ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›ãƒ†ã‚¹ãƒˆ: {'æœ‰åŠ¹' if args.scale_test else 'ç„¡åŠ¹'}")
    print(f"ãƒ‡ãƒã‚¤ã‚¹: {DEVICE}")

    try:
        # åˆ†æå™¨ä½œæˆ
        analyzer = EnhancedGPMaskRatio02Analyzer(args.smoothness)

        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        if not analyzer.load_models():
            return

        # é€šå¸¸ã®èª¤å·®è¨ˆç®—
        analyzer.calculate_all_errors()
        analyzer.calculate_relative_change()

        # ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›ãƒ†ã‚¹ãƒˆ
        if args.scale_test:
            analyzer.scale_transform_test(scale_factors=args.scale_factors, n_test_samples=args.n_scale_samples)
            analyzer.analyze_scale_results()
            analyzer.create_scale_analysis_plots()

        # åˆ†æå®Ÿè¡Œ
        analysis_results = analyzer.create_analysis_plots()
        analyzer.print_detailed_summary(analysis_results)

        print("\n" + "="*60)
        print("ã‚¬ã‚¦ã‚¹éç¨‹åˆ†æå®Œäº†!")
        print("="*60)

    except FileNotFoundError as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()